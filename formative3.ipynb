{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Bayesian Probability "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Formula:\n",
        "-P(A‚à£B) = Posterior probability (probability of A given B)\n",
        "-P(B‚à£A) = Likelihood (probability of B given A)\n",
        "\n",
        "-ùëÉ(ùê¥)P(A) = Prior probability of A\n",
        "\n",
        "-P(B) = Marginal probability of B (normalizing factor)\n",
        "\n",
        "-A be \"the email is spam\"\n",
        "-B be \"the words in the email\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì© Predicting message: \"We have a cheap offer now\"\n",
            "Words: ['we', 'have', 'a', 'cheap', 'offer', 'now']\n",
            "\n",
            "üîç Label: 'spam'\n",
            "  ‚û§ Prior P(spam) = 0.4000\n",
            "  ‚û§ log(Prior) = -0.9163\n",
            "    üîπ P('we'|spam) = 0.0526 ‚Üí log = -2.9444\n",
            "    üîπ P('have'|spam) = 0.0500 ‚Üí log = -2.9957\n",
            "    üîπ P('a'|spam) = 0.0476 ‚Üí log = -3.0445\n",
            "    üîπ P('cheap'|spam) = 0.0952 ‚Üí log = -2.3514\n",
            "    üîπ P('offer'|spam) = 0.0952 ‚Üí log = -2.3514\n",
            "    üîπ P('now'|spam) = 0.0952 ‚Üí log = -2.3514\n",
            "  ‚úÖ Total log score for 'spam': -16.9551\n",
            "\n",
            "üîç Label: 'not_spam'\n",
            "  ‚û§ Prior P(not_spam) = 0.6000\n",
            "  ‚û§ log(Prior) = -0.5108\n",
            "    üîπ P('we'|not_spam) = 0.0500 ‚Üí log = -2.9957\n",
            "    üîπ P('have'|not_spam) = 0.0476 ‚Üí log = -3.0445\n",
            "    üîπ P('a'|not_spam) = 0.0455 ‚Üí log = -3.0910\n",
            "    üîπ P('cheap'|not_spam) = 0.0435 ‚Üí log = -3.1355\n",
            "    üîπ P('offer'|not_spam) = 0.0417 ‚Üí log = -3.1781\n",
            "    üîπ P('now'|not_spam) = 0.0400 ‚Üí log = -3.2189\n",
            "  ‚úÖ Total log score for 'not_spam': -19.1745\n",
            "\n",
            "üî¢ P(spam | message) = 0.9020\n",
            "üî¢ P(not_spam | message) = 0.0980\n",
            "\n",
            "üèÅ Final prediction: SPAM\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'spam'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Training dataset\n",
        "emails = [\n",
        "    (\"Buy cheap meds now\", \"spam\"),\n",
        "    (\"Limited offer just for you\", \"spam\"),\n",
        "    (\"Meeting tomorrow at 10am\", \"not_spam\"),\n",
        "    (\"Project deadline extended\", \"not_spam\"),\n",
        "    (\" No meeting today\", \"not_spam\"),\n",
        "]\n",
        "\n",
        "# Changes a word to lower case and splits\n",
        "def word_splitter(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Count words\n",
        "word_counts = {\n",
        "    \"spam\": defaultdict(int),\n",
        "    \"not_spam\": defaultdict(int)\n",
        "}\n",
        "class_counts = defaultdict(int)\n",
        "\n",
        "for message, label in emails:\n",
        "    class_counts[label] += 1\n",
        "    for word in word_splitter(message):\n",
        "        word_counts[label][word] += 1\n",
        "\n",
        "# Total number of messages\n",
        "total_messages = sum(class_counts.values())\n",
        "\n",
        "# Calculate prior probability of spam or not_spam\n",
        "priors = {\n",
        "    label: count / total_messages\n",
        "    for label, count in class_counts.items()\n",
        "}\n",
        "\n",
        "# Calculate conditional probabilities with Laplace smoothing\n",
        "def word_prob(word, label):\n",
        "    return (word_counts[label][word] + 1) / (sum(word_counts[label].values()) + len(word_counts[label]))\n",
        "\n",
        "# Modified Naive Bayes classifier with detailed logs\n",
        "def predict_verbose(message):\n",
        "    words = word_splitter(message)\n",
        "    scores = {}\n",
        "\n",
        "    print(f\"\\nüì© Predicting message: \\\"{message}\\\"\\nWords: {words}\\n\")\n",
        "\n",
        "    for label in class_counts:\n",
        "        print(f\"üîç Label: '{label}'\")\n",
        "\n",
        "        prior = priors[label]\n",
        "        log_prob = math.log(prior)\n",
        "        print(f\"  ‚û§ Prior P({label}) = {prior:.4f}\")\n",
        "        print(f\"  ‚û§ log(Prior) = {log_prob:.4f}\")\n",
        "\n",
        "        for word in words:\n",
        "            cond_prob = word_prob(word, label)\n",
        "            log_word_prob = math.log(cond_prob)\n",
        "            log_prob += log_word_prob\n",
        "            print(f\"    üîπ P('{word}'|{label}) = {cond_prob:.4f} ‚Üí log = {log_word_prob:.4f}\")\n",
        "\n",
        "        scores[label] = log_prob\n",
        "        print(f\"  ‚úÖ Total log score for '{label}': {log_prob:.4f}\\n\")\n",
        "\n",
        "    # Convert log scores to real probabilities\n",
        "    max_log = max(scores.values())\n",
        "    exp_scores = {label: math.exp(scores[label] - max_log) for label in scores}\n",
        "    total = sum(exp_scores.values())\n",
        "    probs = {label: exp_scores[label] / total for label in exp_scores}\n",
        "\n",
        "    for label in probs:\n",
        "        print(f\"üî¢ P({label} | message) = {probs[label]:.4f}\")\n",
        "\n",
        "    prediction = max(probs, key=probs.get)\n",
        "    print(f\"\\nüèÅ Final prediction: {prediction.upper()}\")\n",
        "    return prediction\n",
        "\n",
        "# Test email\n",
        "test_email = \"We have a cheap offer now\"\n",
        "predict_verbose(test_email)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![alt text](<CamScanner ‚Å®6-22-25 18.57‚Å©_1.jpg>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](<CamScanner ‚Å®6-22-25 18.57‚Å©_1-1.jpg>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](<CamScanner ‚Å®6-22-25 18.57‚Å©_3.jpg>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](<CamScanner ‚Å®6-22-25 18.57‚Å©_4.jpg>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](<CamScanner ‚Å®6-22-25 18.57‚Å©_5.jpg>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Part 4: Gradient Descent in Code "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*In this section we are going to convert the manual calculation computed in part 3 into Pyhon code using SciPy.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import libraries:\n",
        "- numpy: Used for  for numerical computations.\n",
        "- matplotlib.pyplot:Used for creating visualizations and plotting data.\n",
        "-  scipy.optimize.approx_fprime:Computes numerically gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import approx_fprime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialization of Dataset and Parameters\n",
        "\n",
        "1. Initial m=-1\n",
        "2. Initial b=1\n",
        "3. learning rate=0.1\n",
        "4. Given points: (1,3) and (3,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "m=-1\n",
        "b=1\n",
        "alpha = 000.1  # Learning rate\n",
        "\n",
        "# Dataset\n",
        "X = np.array([1, 3]) \n",
        "Y = np.array([3, 6])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Squared Error Cost Function\n",
        "It helps us measure how large the prediction error is.\n",
        "\n",
        "### Components:\n",
        "1. len(x): count how many data point we have in x which in our case is 2 as x ha 2 elements ([1,3])\n",
        "2. prediction :\n",
        "3. error : difference between prediction and true y value.\n",
        "4. cost: Mean Squared Error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cost_function(m, b, X, Y):\n",
        "    N = len(X)\n",
        "    predictions = m * X + b\n",
        "    error = predictions - Y\n",
        "    cost = (1 / N) * np.sum(error ** 2)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Calculation:\n",
        "\n",
        "We define a function to compute the gradients of the cost function with respect to parameters `m`  and `b` .  \n",
        "These gradients tell us how to adjust `m` and `b` in order to reduce the error.\n",
        "\n",
        "### Components:\n",
        "\n",
        "- GoM: tells us how to update m\n",
        "- GoB: tells us how to update b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Calculation\n",
        "def gradients(m, b, X, Y):\n",
        "    N = len(X)\n",
        "    predictions = m * X + b\n",
        "    error = predictions - Y\n",
        "    GoM = (2 / N) * np.sum(error * X)\n",
        "    GoB = (2 / N) * np.sum(error)\n",
        "    return GoM,GoB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Gradient Descent\n",
        "max_iterations =4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: Cost=36.5000, m=1.7000, b=2.1000\n",
            "Iteration 2: Cost=1.0400, m=1.2600, b=1.9000\n",
            "Iteration 3: Cost=0.0640, m=1.3400, b=1.9160\n",
            "Iteration 4: Cost=0.0348, m=1.3336, b=1.8968\n"
          ]
        }
      ],
      "source": [
        "for i in range(max_iterations):\n",
        "    current_cost = cost_function(m, b, X, Y)\n",
        "    GoM, GoB = gradients(m, b, X, Y)\n",
        "    \n",
        "    m = m - alpha * GoM\n",
        "    b = b - alpha * GoB\n",
        "    \n",
        "    print(f\"Iteration {i+1}: Cost={current_cost:.4f}, m={m:.4f}, b={b:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
